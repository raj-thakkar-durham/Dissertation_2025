# Step-by-Step Coding Guide: Hybrid Gold Price Prediction Simulation

## Phase 1: Data Collection & Preprocessing

### Step 1.1: Setup Environment and Dependencies
```python
# Install required packages first
pip install pandas numpy yfinance vaderSentiment scikit-learn mesa matplotlib seaborn requests beautifulsoup4 numba joblib

# Create main project structure
project_folder/
├── data/
├── models/
├── utils/
├── results/
└── main.py
```

### Step 1.2: Historical Data Collection
**File: `data/data_collector.py`**
```python
import pandas as pd
import yfinance as yf
from datetime import datetime, timedelta
import numpy as np

class DataCollector:
    def __init__(self, start_date, end_date):
        self.start_date = start_date
        self.end_date = end_date
        
    def fetch_gold_data(self):
        # Fetch gold prices (GLD ETF or GC=F futures)
        # Return DataFrame with Date, Open, High, Low, Close, Volume
        pass
    
    def fetch_oil_data(self):
        # Fetch Brent oil prices (BZ=F) or WTI (CL=F)
        # Return DataFrame with Date, Close
        pass
    
    def fetch_market_indices(self):
        # Fetch S&P 500 (^GSPC) and USD Index (DX-Y.NYB)
        # Return DataFrame with Date, Close for each index
        pass
    
    def merge_market_data(self):
        # Combine all market data into single DataFrame
        # Handle missing values with forward fill or interpolation
        # Return merged DataFrame indexed by Date
        pass
```

### Step 1.3: News Sentiment Collection
**File: `data/sentiment_analyzer.py`**
```python
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
import requests
from bs4 import BeautifulSoup
import pandas as pd

class SentimentAnalyzer:
    def __init__(self):
        self.analyzer = SentimentIntensityAnalyzer()
        
    def fetch_news_headlines(self, date):
        # Implement news API calls or web scraping
        # Return list of headlines for given date
        pass
    
    def calculate_daily_sentiment(self, headlines):
        # Process headlines through VADER
        # Return compound sentiment score (-1 to 1)
        pass
    
    def generate_sentiment_series(self, date_range):
        # Generate daily sentiment scores for entire date range
        # Return DataFrame with Date, Sentiment
        pass
```

### Step 1.4: Feature Engineering
**File: `utils/feature_engineering.py`**
```python
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler

class FeatureEngineering:
    def __init__(self, data):
        self.data = data
        self.scaler = StandardScaler()
        
    def calculate_returns(self):
        # Calculate log returns for gold and oil
        # Add columns: gold_return, oil_return
        pass
    
    def calculate_moving_averages(self, windows=[5, 10, 20]):
        # Calculate moving averages for gold prices
        # Add columns: gold_ma_5, gold_ma_10, gold_ma_20
        pass
    
    def calculate_volatility(self, window=20):
        # Calculate rolling volatility
        # Add column: gold_volatility
        pass
    
    def create_ca_grid_features(self):
        # Create 3x3 grid features for CA
        # Center: current gold return
        # 8 neighbors: past 4 days returns + 4 days oil/sentiment
        pass
    
    def normalize_features(self):
        # Normalize all features to comparable scales
        # Return normalized DataFrame
        pass
```

## Phase 2: Cellular Automaton Implementation

### Step 2.1: CA Grid Structure
**File: `models/cellular_automaton.py`**
```python
import numpy as np
from numba import jit

class CellularAutomaton:
    def __init__(self, grid_size=(50, 50), num_states=3):
        self.grid_size = grid_size
        self.num_states = num_states  # e.g., -1, 0, 1 for bearish, neutral, bullish
        self.grid = np.zeros(grid_size, dtype=int)
        self.rules = {}
        
    def initialize_grid(self, initial_state):
        # Initialize grid based on market features
        # Map features to discrete states (-1, 0, 1)
        pass
    
    @jit(nopython=True)
    def get_neighbors(self, x, y):
        # Get 8-neighborhood values for cell at (x, y)
        # Handle boundary conditions
        pass
    
    def define_rules(self, rule_parameters):
        # Define CA update rules based on neighbor configurations
        # Rules map 8-neighbor pattern to new center state
        pass
    
    def update_cell(self, x, y, external_features):
        # Update single cell based on neighbors and external inputs
        # Return new state for cell
        pass
    
    def step(self, external_features):
        # Update entire grid for one time step
        # Use vectorized operations for efficiency
        pass
    
    def get_market_signal(self):
        # Aggregate grid state into market signal
        # Return value between -1 and 1
        pass
```

### Step 2.2: CA Rule Optimization
**File: `models/ca_optimizer.py`**
```python
import numpy as np
from scipy.optimize import differential_evolution
from joblib import Parallel, delayed

class CARuleOptimizer:
    def __init__(self, ca_model, historical_data):
        self.ca_model = ca_model
        self.historical_data = historical_data
        
    def objective_function(self, rule_parameters):
        # Evaluate how well CA rules predict historical price movements
        # Return error metric (lower is better)
        pass
    
    def optimize_rules(self, method='differential_evolution'):
        # Optimize CA rule parameters to fit historical data
        # Use parallel evaluation for speed
        pass
    
    def validate_rules(self, test_data):
        # Test optimized rules on out-of-sample data
        # Return validation metrics
        pass
```

## Phase 3: Agent-Based Model Implementation

### Step 3.1: Agent Classes
**File: `models/agents.py`**
```python
from mesa import Agent
import numpy as np
import random

class GoldTrader(Agent):
    def __init__(self, unique_id, model, agent_type='contrarian'):
        super().__init__(unique_id, model)
        self.agent_type = agent_type
        self.position = 0  # -1: short, 0: neutral, 1: long
        self.cash = 10000
        self.gold_holdings = 0
        self.risk_tolerance = random.uniform(0.1, 0.9)
        self.sentiment_bias = random.uniform(-0.5, 0.5)
        
    def observe_environment(self):
        # Observe CA signals and neighbor actions
        # Return dictionary of observations
        pass
    
    def make_decision(self, observations):
        # Decision logic based on agent type and observations
        # Return action: 'buy', 'sell', or 'hold'
        pass
    
    def execute_trade(self, action, price):
        # Execute the decided action
        # Update position, cash, and holdings
        pass
    
    def step(self):
        # Agent's step function called each simulation day
        pass

class HerderAgent(GoldTrader):
    def __init__(self, unique_id, model):
        super().__init__(unique_id, model, agent_type='herder')
        
    def make_decision(self, observations):
        # Follow majority of neighbors
        pass

class ContrarianAgent(GoldTrader):
    def __init__(self, unique_id, model):
        super().__init__(unique_id, model, agent_type='contrarian')
        
    def make_decision(self, observations):
        # Act opposite to majority
        pass

class TrendFollowerAgent(GoldTrader):
    def __init__(self, unique_id, model):
        super().__init__(unique_id, model, agent_type='trend_follower')
        
    def make_decision(self, observations):
        # Follow price trends and momentum
        pass
```

### Step 3.2: Market Model
**File: `models/market_model.py`**
```python
from mesa import Model
from mesa.time import RandomActivation
from mesa.space import MultiGrid
from mesa.datacollection import DataCollector
import numpy as np

class GoldMarketModel(Model):
    def __init__(self, num_agents, grid_size, ca_model):
        super().__init__()
        self.num_agents = num_agents
        self.grid = MultiGrid(grid_size, grid_size, True)
        self.schedule = RandomActivation(self)
        self.ca_model = ca_model
        self.current_price = 1800  # Starting gold price
        self.price_history = []
        self.external_data = None
        self.current_day = 0
        
        # Create agents
        self.create_agents()
        
        # Data collection
        self.datacollector = DataCollector(
            model_reporters={"Price": "current_price", "Volume": self.get_volume},
            agent_reporters={"Position": "position", "Cash": "cash"}
        )
        
    def create_agents(self):
        # Create different types of agents
        # Distribute them on grid
        pass
    
    def update_price(self):
        # Calculate net demand from all agents
        # Update price based on supply/demand
        pass
    
    def get_volume(self):
        # Calculate total trading volume
        pass
    
    def step(self):
        # One simulation day
        # Update CA, agents act, update price
        pass
    
    def run_simulation(self, num_days, external_data):
        # Run complete simulation
        # Return price series and metrics
        pass
```

## Phase 4: Integration and Simulation Process

### Step 4.1: Main Simulation Engine
**File: `main_simulation.py`**
```python
import pandas as pd
import numpy as np
from models.cellular_automaton import CellularAutomaton
from models.market_model import GoldMarketModel
from data.data_collector import DataCollector
from utils.feature_engineering import FeatureEngineering
import multiprocessing as mp

class HybridGoldSimulation:
    def __init__(self, config):
        self.config = config
        self.data = None
        self.ca_model = None
        self.market_model = None
        self.results = {}
        
    def load_and_prepare_data(self):
        # Load historical data
        # Apply feature engineering
        # Split into train/test sets
        pass
    
    def initialize_models(self):
        # Initialize CA model
        # Initialize ABM model
        # Connect models
        pass
    
    def calibrate_models(self):
        # Optimize CA rules on training data
        # Calibrate agent parameters
        pass
    
    def run_single_simulation(self, seed=None):
        # Run one complete simulation
        # Return simulated price series
        pass
    
    def run_parallel_simulations(self, num_runs=100):
        # Run multiple simulations in parallel
        # Return ensemble of results
        pass
    
    def validate_results(self):
        # Compare simulated vs actual prices
        # Calculate error metrics
        pass
```

### Step 4.2: Parallel Execution Framework
**File: `utils/parallel_runner.py`**
```python
from joblib import Parallel, delayed
import multiprocessing as mp
from mesa.batchrunner import BatchRunner

class ParallelSimulationRunner:
    def __init__(self, model_class, num_cores=None):
        self.model_class = model_class
        self.num_cores = num_cores or mp.cpu_count()
        
    def run_batch_simulations(self, parameters, num_iterations):
        # Use Mesa's BatchRunner for parallel execution
        pass
    
    def run_parameter_sweep(self, parameter_ranges):
        # Run simulations across parameter space
        # Return results DataFrame
        pass
    
    def optimize_parameters(self, objective_function):
        # Parallel parameter optimization
        pass
```

## Phase 5: Analysis and Validation

### Step 5.1: Results Analysis
**File: `analysis/results_analyzer.py`**
```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import mean_squared_error, mean_absolute_error

class ResultsAnalyzer:
    def __init__(self, simulation_results, actual_data):
        self.results = simulation_results
        self.actual = actual_data
        
    def calculate_metrics(self):
        # RMSE, MAE, MAPE, directional accuracy
        pass
    
    def plot_price_comparison(self):
        # Plot simulated vs actual prices
        pass
    
    def analyze_agent_behavior(self):
        # Analyze agent position changes over time
        pass
    
    def generate_report(self):
        # Generate comprehensive analysis report
        pass
```

### Step 5.2: Visualization Tools
**File: `visualization/plots.py`**
```python
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.graph_objects as go
from plotly.subplots import make_subplots

class VisualizationTools:
    def __init__(self):
        self.setup_style()
        
    def setup_style(self):
        # Set up plotting style
        pass
    
    def plot_simulation_results(self, results):
        # Interactive plots of simulation results
        pass
    
    def plot_ca_evolution(self, ca_states):
        # Visualize CA grid evolution over time
        pass
    
    def create_dashboard(self, results):
        # Create interactive dashboard
        pass
```

## Implementation Order and Tips

### Phase 1: Start Here (Week 1-2)
1. **Begin with data collection** - get the basic data pipeline working
2. **Implement feature engineering** - create the market features
3. **Test data preprocessing** - ensure data quality

### Phase 2: Core Models (Week 3-4)
1. **Build simple CA first** - start with basic rules
2. **Create basic agents** - implement one agent type initially
3. **Test integration** - make sure CA and ABM communicate

### Phase 3: Enhancement (Week 5-6)
1. **Add parallelization** - optimize for speed
2. **Implement optimization** - tune parameters
3. **Add multiple agent types** - increase complexity

### Phase 4: Validation (Week 7-8)
1. **Run simulations** - generate results
2. **Validate against historical data** - check accuracy
3. **Create visualizations** - analyze results

## Key Copilot Prompts to Use

For each file, use these specific prompts with Copilot:

1. **Data Collection**: "Implement yfinance data fetching for gold prices with error handling and data validation"

2. **Feature Engineering**: "Create rolling window features for time series data with proper handling of missing values"

3. **CA Implementation**: "Implement cellular automaton with 8-neighborhood rules and vectorized updates using NumPy"

4. **Agent Classes**: "Create Mesa agent classes with decision-making logic based on market observations"

5. **Parallel Processing**: "Implement parallel simulation runs using joblib and multiprocessing"

6. **Optimization**: "Create parameter optimization using scipy.optimize with parallel evaluation"

7. **Analysis**: "Generate comprehensive analysis with error metrics and statistical tests"

## Testing Strategy

1. **Unit Tests**: Test each component individually
2. **Integration Tests**: Test CA-ABM interaction
3. **Performance Tests**: Measure execution speed
4. **Validation Tests**: Compare against known results

Start with the data collection phase and work through each step systematically. Each phase builds on the previous one, so ensure each component works before moving to the next.